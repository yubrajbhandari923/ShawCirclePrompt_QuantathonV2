{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e74680ab",
   "metadata": {},
   "source": [
    "# Quantum PCA for Financial Dataset\n",
    "### **Learning Objectives**\n",
    "1. Why use Principal Component Analysis (PCA)? How to use PCA?\n",
    "2. Apply PCA to a dataset. Form statistical conclusions per results.\n",
    "3. Understand and mitigate the impact of outliers and noisy data.\n",
    "3. Why use Quantum Annealing (QA)? How to use QA?\n",
    "4. Experiment with implementing a QA algorithm.\n",
    "\n",
    "#### **Principal Component Analysis**\n",
    "*Principal Component Analysis* (PCA) is a common data science procedure used for extracting key features (aka principal components) from a dataset. Knowing the principal components is helpful for dimensionality reduction, denoising, and visualization.  \n",
    "For an intuitive understanding of PCA, here is a [great rundown from IBM](https://www.ibm.com/think/topics/principal-component-analysis) and youtube has a plethora of instructive videos. Develop an understanding of the fundamentals before continuing.  \n",
    "Understand both the recursive formulation of PCA and the implementation that solves for several principal components simultaneously.  \n",
    "\n",
    "### **Quantum Annealing**\n",
    "*Quantum Annealing* (QA) is an optimization algorithm for finding the global minimum of a solution set. It works by creating quantum fluctations in input data to explore the dataset.  \n",
    "For an intuitive understanding of QA, there are also some youtube videos that have great visualizers. Here is a [great video](https://youtu.be/zvfkXjzzYOo?si=t1X2Nl-277pAQDV4) from D-Wave.\n",
    "\n",
    "### **QaPCA**\n",
    "*Quantum Annealing for Robust Principal Component Analysis* ([pdf](https://arxiv.org/pdf/2501.10431)), published in December 2024, introduced an implementation of PCA using Quantum Annealing. The goal of the article was to develop a *robust* implementation, i.e. an implementation that mitigated the impact of noise.  \n",
    "\n",
    "### **Problem Statement - Detecting and Analyzing Financial Principal Components with QAPCA**\n",
    "This challenge will have competitors build L1-PCA classically, followed by QAPCA-R, in order to visualize the natural groupings of different tickers into sectors from price movements. Competitors will be tested on how robustly their implementation manages noise from both the quantum system and the data itself.\n",
    "##### Input: Time series of stock prices\n",
    "Steps:\n",
    "1. Pre-process data\n",
    "2. Apply PCA classically. \n",
    "3. Analyze results.\n",
    "4. Construct Ising Model for Recursive QaPCA.\n",
    "5. Analyze results. How do they compare with classical PCA?\n",
    "6. Bonus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2630864c",
   "metadata": {},
   "source": [
    "## Step 0: Downloading data\n",
    "We will be utilizing stock price data from 2018/01/01 to 2018/03/01 for various different tickers from different sectors. The list of tickers and their corresponding sector are listed as follows:\n",
    "| Ticker | Company Name           | Sector         |\n",
    "|--------|------------------------|----------------|\n",
    "| AAPL   | Apple Inc.             | Technology     |\n",
    "| AMZN   | Amazon.com Inc.        | Technology     |\n",
    "| GOOG   | Alphabet Inc Class C   | Technology     |\n",
    "| MSFT   | Microsoft Corporation  | Technology     |\n",
    "| XOM    | Exxon Mobil Corp       | Energy         |\n",
    "| GLD    | SPDR Gold Trust        | Finance        |\n",
    "| AEP    | Amer. Elec. Power Comp | Utility        |\n",
    "| DUK    | Duke Energy Corp       | Utility        |\n",
    "| SO     | Southern Co            | Utility        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0ad8d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle imports\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "126d642a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from local CSV file.\n"
     ]
    }
   ],
   "source": [
    "tickers_path = 'data/prices.csv'\n",
    "if os.path.exists(tickers_path):\n",
    "    print(\"Loading data from local CSV file.\")\n",
    "    ticker_data = pd.read_csv(tickers_path)\n",
    "else:\n",
    "    tickers = ['GOOG', 'XOM', 'AAPL', 'AMZN', 'GLD', 'DUK', 'SO', 'AEP']\n",
    "    date_range = ['2017-01-01', '2017-03-01']\n",
    "\n",
    "    ticker_data = yf.download(\n",
    "        tickers,\n",
    "        start=date_range[0],\n",
    "        end=date_range[1],\n",
    "        group_by=\"ticker\",\n",
    "        auto_adjust=True\n",
    "    )\n",
    "    ticker_data = ticker_data.stack(level=0).rename_axis(['Date', 'Ticker']).reset_index()\n",
    "\n",
    "    ticker_data.to_csv(tickers_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe20b5f",
   "metadata": {},
   "source": [
    "## Step 1: Preparing data\n",
    "For each ticker, we will look at their daily log returns over the period 2018/01/01 to 2018/03/01. We will treat this list of returns as each ticker's \"feature\", which acts as a descriptor for the ticker. Compute the daily difference between log returns for each ticker over the provided time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "679dfc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ticker\n",
       "AAPL    [-0.0011199028902573013, 0.00507214869159839, ...\n",
       "AEP     [0.0014331568900181704, 0.005394855084582408, ...\n",
       "AMZN    [0.00464647301631583, 0.030269616426223996, 0....\n",
       "DUK     [-0.001029763606802401, 0.0012867048462190133,...\n",
       "GLD     [0.0035241475629946054, 0.015395946803090798, ...\n",
       "GOOG    [0.0009665000493054286, 0.009007379345297757, ...\n",
       "SO      [-0.0010201691831062953, 0.0030571665019612877...\n",
       "XOM     [-0.011063485130485693, -0.015019433359217812,...\n",
       "Name: LogReturn, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code\n",
    "tickers = ticker_data['Ticker'].unique().tolist()\n",
    "\n",
    "# Calculate the daily returns for each ticker by pt/pt-1 - 1\n",
    "ticker_data = ticker_data.sort_values(by=['Ticker', 'Date'])\n",
    "ticker_data[\"Return\"] = ticker_data.groupby('Ticker')['Close'].pct_change()\n",
    "ticker_data['Return'] = ticker_data['Return'].fillna(0)\n",
    "ticker_data['LogReturn'] = np.log1p(ticker_data['Return'])\n",
    "\n",
    "# For each ticker, get the vector of LogReturns\n",
    "log_return_vectors = ticker_data.groupby('Ticker')['LogReturn'].apply(np.array)\n",
    "\n",
    "# Remove the zero returns\n",
    "log_return_vectors = log_return_vectors.apply(lambda x: x[x != 0])\n",
    "log_return_vectors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca44e133",
   "metadata": {},
   "source": [
    "## Step 2: Apply PCA classically\n",
    "We will now apply PCA classically on our set of features. Per section 2.2 from the QAPCA paper (linked above), L1-PCA can be defined as the following problem:\n",
    "$$\n",
    "    \\mathbf{\\hat{R}}_{L_1} = \\underset{\\mathbf{R} \\in \\mathbb{R}^{D \\times K}, \\mathbf{R}^T\\mathbf{R=I}_K}{\\operatorname{\\argmax}} ||\\mathbf{R}^T\\mathbf{X}||_1\n",
    "$$\n",
    "where $||*||_1$ is the L1-norm of a vector (sum of absolute entries).  \n",
    "The paper walks through how to transform this into the binary optimization problem\n",
    "$$\n",
    "\\mathbf{\\hat{R}}_{L_1} = \\Phi\\left(\\mathbf{X}\\mathbf{B_{opt}} \\right)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathbf{B_{opt}} = \\underset{\\mathbf{B} \\in \\{\\pm 1\\}^{N \\times K}}{\\operatorname{\\argmax}}\\sum_{k=1}^K {\\sigma}_k \\left[\\mathbf{B}^T \\mathbf{X}^T \\mathbf{XB} \\right]\n",
    "$$\n",
    "Per the paper, $K$ is the number of PCA components, ${\\sigma}_k[*]$ represents the $k^{th}$ singular-value of its argument (derived from SVD), $\\mathbf{B} \\in \\{ \\pm 1\\}^{N \\times K}$ is a binary matrix, and $\\Phi(*)$ returns the nearest orthonormal matrix to its argument (solved using SVD).  \n",
    "For $K = 1$, this expression becomes\n",
    "$$\n",
    "\\mathbf{B_{opt}} = \\underset{\\mathbf{B} \\in \\{\\pm 1\\}^N}{\\operatorname{\\argmax}} ||\\mathbf{X} \\hat{\\mathbf{b}}||_2 = \\underset{\\mathbf{B} \\in \\{\\pm 1\\}^N}{\\operatorname{\\argmax}} \\: \\hat{\\mathbf{b}}^T \\mathbf{X}^T\\mathbf{X} \\hat{\\mathbf{b}} \n",
    "$$\n",
    "This binary quadratic form is formally an [NP-hard problem](https://arxiv.org/abs/2301.06978)!!! These are magic words for quantum nerds. We can then rewrite this formulation in Ising form as:\n",
    "$$\n",
    "\\mathbf{b_{opt}} = \\underset{\\hat{\\mathbf{b}}\\in\\{\\pm 1\\}^N}{\\operatorname{\\argmin}} \\hspace{2mm} \\hat{\\mathbf{b}}^T\\left( - \\mathbf{X}^T\\mathbf{X} \\right)\\hat{\\mathbf{b}} = \\underset{\\hat{\\mathbf{b}}\\in\\{\\pm 1\\}^N}{\\operatorname{\\argmin}} \\hspace{2mm} \\hat{\\mathbf{b}}^T(-\\mathbf{J})\\hat{\\mathbf{b}}\n",
    "$$\n",
    "where $J$ represents the covariance matrix.  \n",
    "\n",
    "We can iteratively anneal and update the covariance matrix to solve for one principal component at a time.  \n",
    "We offer functions and descriptions, as well as the steps for calling. We recommend implementing helper functions yourself instead of using packages. At one step you should use some optimizer package, e.g. dual_annealing from scipy.optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d13d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import dual_annealing\n",
    "\n",
    "def solve_covariance_matrix(sample_data):\n",
    "    \"\"\"Calculate covariance matrix for given sample data\"\"\"\n",
    "    \n",
    "    # For each ticker, get the vector of LogReturns and stack them into a 2D array (features x samples)\n",
    "    stacked_data = np.vstack(sample_data.values)\n",
    "    print(\"Stacked data shape:\", stacked_data.shape)  # Debugging line to check shape\n",
    "    return stacked_data @ stacked_data.T, stacked_data.T\n",
    "\n",
    "\n",
    "def l1_objective(b, J):\n",
    "    \"\"\"\n",
    "    Compute the L1 PCA objective function.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    b : numpy.ndarray\n",
    "        The current component vector (will be reshaped to column vector)\n",
    "    J : numpy.ndarray\n",
    "        The covariance matrix or modified covariance matrix for the optimization\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        The l1 objective function value\n",
    "    \"\"\"\n",
    "    \n",
    "    # bT(-J)b\n",
    "    return b.T @ (-J) @ b\n",
    "\n",
    "def Phi(T):\n",
    "    # T: (D,K). If K=1 this reduces to unit-normalizing the vector.\n",
    "    U, _, Vt = np.linalg.svd(T, full_matrices=False)\n",
    "    return U @ Vt\n",
    "\n",
    "def solve_l1_classical_component(J):\n",
    "    \"\"\"\n",
    "    Solve for a single L1 PCA component using simulated annealing.\n",
    "    \n",
    "    This function finds the k-th principal component by optimizing the L1 objective\n",
    "    function. For k > 0, it ensures orthogonality with previous components by\n",
    "    modifying the covariance matrix to ignore the subspace spanned by previous\n",
    "    components.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    J : numpy.ndarray\n",
    "        The covariance matrix (samples x samples)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        A tuple containing:\n",
    "        - numpy.ndarray: The normalized component vector\n",
    "        - numpy.ndarray: Covariance matrix, now orthogonal to the kth principal component\n",
    "    \"\"\"\n",
    "    n = J.shape[0]\n",
    "    \n",
    "    bounds = [(-1, 1) for _ in range(n)]    \n",
    "    x0 = np.random.uniform(-1, 1, n)\n",
    "    result = dual_annealing(l1_objective, bounds, args=(J,), x0=x0)\n",
    "    \n",
    "    b_opt = result.x\n",
    "    print(\"Optimized b (before normalization):\", b_opt)\n",
    "    \n",
    "    r_norm_sqrd = b_opt.T @ J @ b_opt\n",
    "    # r_norm_sqrd = 1\n",
    "    \n",
    "    bbT = np.outer(b_opt, b_opt)\n",
    "\n",
    "    print(f\"SHapes: J: {J.shape}, bbT: {bbT.shape}, b_opt: {b_opt.shape}\")\n",
    "    J_new = J \n",
    "    J_new = J_new - ((2/r_norm_sqrd) * J @ bbT @ J) \n",
    "    J_new = J_new + ((J @ bbT @ J @ bbT @ J) / (r_norm_sqrd**2))\n",
    "\n",
    "    return b_opt, J_new\n",
    "\n",
    "def do_l1_pca(sample_data, K, get_component_func=solve_l1_classical_component):\n",
    "    \"\"\"Gets K principal components\"\"\"\n",
    "    J, X = solve_covariance_matrix(sample_data)\n",
    "    components = []\n",
    "    r = None\n",
    "\n",
    "    for k in range(K):\n",
    "        r, J = get_component_func(J)\n",
    "        components.append(r)\n",
    "    \n",
    "    Bopt = np.vstack(components).T\n",
    "    \n",
    "    print(\"Bopt shape:\", Bopt.shape)\n",
    "    print(\"X shape:\", X.shape)\n",
    "    \n",
    "    X_Bopt = X @ Bopt\n",
    "    R_L1 = Phi(X_Bopt)\n",
    "    \n",
    "    print(\"R_L1 shape:\", R_L1.shape)\n",
    "    emb = R_L1.T @ X\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a06029b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked data shape: (8, 38)\n",
      "Optimized b (before normalization): [-1.  1. -1.  1.  1. -1.  1.  1.]\n",
      "SHapes: J: (8, 8), bbT: (8, 8), b_opt: (8,)\n",
      "Bopt shape: (8, 1)\n",
      "X shape: (38, 8)\n",
      "R_L1 shape: (38, 1)\n",
      "PCA Embeddings shape: (1, 8)\n"
     ]
    }
   ],
   "source": [
    "ticker_data = log_return_vectors\n",
    "pca_embeddings = do_l1_pca(ticker_data, 1, solve_l1_classical_component)\n",
    "print(\"PCA Embeddings shape:\", pca_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29883b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30c2df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b462d75",
   "metadata": {},
   "source": [
    "## Step 3: Analyze results\n",
    "After doing PCA on our list of vectors, graph the encodings for each ticker and answer some of the following questions regarding the graph and implementation.\n",
    "1. Do sectors naturally emerge from the encodings for each ticker? \n",
    "2. Why did we need to use PCA to visualize how these sectors emerged?\n",
    "3. What does it mean for an algorithm to be *robust*? Why is it important for an algorithm to be robust?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ddf5629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioned Tickers based on L1 PCA:\n",
      "Group 1: ['AEP', 'DUK', 'GLD', 'SO', 'XOM']\n",
      "Group 2: ['AAPL', 'AMZN', 'GOOG']\n"
     ]
    }
   ],
   "source": [
    "# Get the tickers list\n",
    "tickers = log_return_vectors.index.tolist()\n",
    "\n",
    "tickers_partition = (list(), list())\n",
    "for idx, tick in enumerate(tickers):\n",
    "    if pca_embeddings[0,idx] >= 0:\n",
    "        tickers_partition[0].append(tick)\n",
    "    else:\n",
    "        tickers_partition[1].append(tick)\n",
    "\n",
    "# Display the partitioned tickers\n",
    "print(\"Partitioned Tickers based on L1 PCA:\")\n",
    "print(\"Group 1:\", tickers_partition[0])\n",
    "print(\"Group 2:\", tickers_partition[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d62556",
   "metadata": {},
   "source": [
    "#### ***Replace this block with your analysis***\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50349aa",
   "metadata": {},
   "source": [
    "## Step 4: Construct Ising Model for QAPCA-R\n",
    "**Instructions:**  \n",
    "After completing previous steps, we can now consider how we would run this problem in a format that a quantum computer could read. The model given in Eq. 8 of the paper, which is already included in the description for Step 2 of this prompt, is conveniently already very close to an Ising Model! Read more about Ising Models for an intuitive understanding. The coupling strengths of the Ising model are simply the elements of the covariance matrix $J$.  \n",
    "We recommend using the package neal's SimulatedAnnealingSampler for solving the Ising Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "628fcc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neal\n",
    "\n",
    "def convert_J_to_ising_model(X):\n",
    "    \"\"\"\n",
    "    Converts covariance matrix J into dict of Ising Model couplings.\n",
    "    \"\"\"\n",
    "    ising_model = {}\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X)):\n",
    "            if i < j:\n",
    "                ising_model[(i, j)] =  -X[i, j]\n",
    "    return ising_model\n",
    "\n",
    "def solve_l1_qapca_r_component(J):\n",
    "    \"\"\"\n",
    "    Solve for a single L1 QAPCA component using simulated annealing.\n",
    "    \n",
    "    This function finds the k-th principal component by optimizing the L1 objective\n",
    "    function. For k > 0, it ensures orthogonality with previous components by\n",
    "    modifying the covariance matrix to ignore the subspace spanned by previous\n",
    "    components.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    J : numpy.ndarray\n",
    "        The covariance matrix (samples x samples)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        A tuple containing:\n",
    "        - numpy.ndarray: The normalized component vector\n",
    "        - numpy.ndarray: Covariance matrix, now orthogonal to the kth principal component\n",
    "    \"\"\"\n",
    "    sampler = neal.SimulatedAnnealingSampler()\n",
    "    \n",
    "    h = {i: 0 for i in range(J.shape[0])}\n",
    "    J_ising = convert_J_to_ising_model(J)\n",
    "    \n",
    "    sampleset = sampler.sample_ising(h, J_ising, num_reads=100)\n",
    "    best_sample = sampleset.first.sample\n",
    "    \n",
    "    b_opt = np.array([best_sample[i] for i in range(J.shape[0])])\n",
    "    r_norm_sqrd = b_opt.T @ J @ b_opt\n",
    "    bbT = np.outer(b_opt, b_opt)\n",
    "    \n",
    "    print(f\"SHapes: J: {J.shape}, bbT: {bbT.shape}, b_opt: {b_opt.shape}\")\n",
    "    J_new = J\n",
    "    J_new = J_new - ((2/r_norm_sqrd) * J @ bbT @ J) \n",
    "    J_new = J_new + ((J @ bbT @ J @ bbT @ J) / (r_norm_sqrd**2))   \n",
    "    \n",
    "    return b_opt, J_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dea7fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked data shape: (8, 38)\n",
      "SHapes: J: (8, 8), bbT: (8, 8), b_opt: (8,)\n",
      "Bopt shape: (8, 1)\n",
      "X shape: (38, 8)\n",
      "R_L1 shape: (38, 1)\n"
     ]
    }
   ],
   "source": [
    "qapca_r_embeddings = do_l1_pca(ticker_data, 1, solve_l1_qapca_r_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386e5e70",
   "metadata": {},
   "source": [
    "## Step 5: Analyze results\n",
    "Analyze your results! Include visuals and brief explanations for what the visuals represent. Also answer some of these questions:  \n",
    "1. How does Quantum Annealing work?\n",
    "2. What is an Ising Model?\n",
    "3. What are key differences between quantum and classical PCA with regards to results and implementation? Are the results guaranteed to be the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18ec7beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioned Tickers based on L1 PCA:\n",
      "Group 1: ['AEP', 'DUK', 'GLD', 'SO', 'XOM']\n",
      "Group 2: ['AAPL', 'AMZN', 'GOOG']\n"
     ]
    }
   ],
   "source": [
    "# code\n",
    "tickers = log_return_vectors.index.tolist()\n",
    "\n",
    "tickers_partition = (list(), list())\n",
    "for idx, tick in enumerate(tickers):\n",
    "    if qapca_r_embeddings[0, idx] >= 0:\n",
    "        tickers_partition[0].append(tick)\n",
    "    else:\n",
    "        tickers_partition[1].append(tick)\n",
    "\n",
    "# Display the partitioned tickers\n",
    "print(\"Partitioned Tickers based on L1 PCA:\")\n",
    "print(\"Group 1:\", tickers_partition[0])\n",
    "print(\"Group 2:\", tickers_partition[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fae86b",
   "metadata": {},
   "source": [
    "#### ***Replace with your analysis***\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed71b672",
   "metadata": {},
   "source": [
    "## **BONUS:** \n",
    "Dig deeper into PCA, QA, and/or QAPCA! With any leftover time, explore alternative implementations and applications. We have listed out some ideas, but explore whatever inspires your team!  \n",
    "### Ideas:  \n",
    "1. **Test implementation on bigger dataset!** *Difficulty: 1/5*\n",
    "2. **Add outliers and test robustness!** *Difficulty: 2/5*\n",
    "3. **Implement your own annealing algorithm!** *Difficulty: 3/5*\n",
    "4. **Implement multi-component QAPCA** (section 3.2 of article)**!** *Difficulty: 5/5*\n",
    "5. **Run this on a quantum computer!** *Difficulty: 5/5*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9488c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f779d09f",
   "metadata": {},
   "source": [
    "## **Final Step:** Present your findings and pitch your implementation. Why would a financial firm benefit from using your implementation?  \n",
    "Make sure to submit your presentation on time per QuantathonV2 rules! Understand that we don't care if you use AI, but we do very much care that everyone on your team learns about PCA, QA, and QAPCA!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9738b",
   "metadata": {},
   "source": [
    "# End of challenge. Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
